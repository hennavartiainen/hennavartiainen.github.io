<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Henna Vartiainen">
<meta name="dcterms.date" content="2023-05-18">

<title>Introduction to clustering in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#what-is-clustering" id="toc-what-is-clustering" class="nav-link active" data-scroll-target="#what-is-clustering">What is clustering?</a></li>
  <li><a href="#example-dataset" id="toc-example-dataset" class="nav-link" data-scroll-target="#example-dataset">Example dataset</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering">Hierarchical clustering</a>
  <ul class="collapse">
  <li><a href="#calculating-the-dissimilarity-matrix" id="toc-calculating-the-dissimilarity-matrix" class="nav-link" data-scroll-target="#calculating-the-dissimilarity-matrix">1. Calculating the dissimilarity matrix</a></li>
  <li><a href="#determining-the-linkage-type" id="toc-determining-the-linkage-type" class="nav-link" data-scroll-target="#determining-the-linkage-type">2. Determining the linkage type</a></li>
  <li><a href="#running-hierarchical-cluster-analysis" id="toc-running-hierarchical-cluster-analysis" class="nav-link" data-scroll-target="#running-hierarchical-cluster-analysis">3. Running hierarchical cluster analysis</a></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization">4. Visualization</a></li>
  </ul></li>
  <li><a href="#k-means" id="toc-k-means" class="nav-link" data-scroll-target="#k-means">K-means</a>
  <ul class="collapse">
  <li><a href="#determining-the-optimal-number-of-clusters" id="toc-determining-the-optimal-number-of-clusters" class="nav-link" data-scroll-target="#determining-the-optimal-number-of-clusters">1. Determining the optimal number of clusters</a></li>
  <li><a href="#running-k-means-analysis" id="toc-running-k-means-analysis" class="nav-link" data-scroll-target="#running-k-means-analysis">2. Running k-means analysis</a></li>
  <li><a href="#visualization-1" id="toc-visualization-1" class="nav-link" data-scroll-target="#visualization-1">3. Visualization</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction to clustering in R</h1>
  <div class="quarto-categories">
    <div class="quarto-category">code</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Henna Vartiainen </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 18, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="what-is-clustering" class="level1">
<h1>What is clustering?</h1>
<p>Clustering is a machine learning technique that helps us group similar data points together without knowing the group labels in advance. It’s like sorting objects based on their similarities rather than their names or categories. There are two main types of clustering:</p>
<p><b>Hard Clustering:</b> In hard clustering, each data point is assigned to only one cluster. It’s like putting each object into a specific box or group based on its similarity to other objects in that group. The goal is to create distinct and non-overlapping clusters. Common examples of hard clustering include K-means, Hierarchical clustering, and DBSCAN.</p>
<p><b>Soft Clustering:</b> In soft clustering, also known as fuzzy clustering, data points can belong to multiple clusters with different degrees of membership. It’s like assigning each object a score for how much it belongs to different groups. This allows for more flexibility in capturing complex relationships and overlapping patterns in the data. Common examples of hard clustering include Fuzzy C-means (FCM) and Gaussian Mixture Models (GMM)</p>
<p>Both types of clustering have their uses depending on the specific problem and data characteristics. They help us discover hidden patterns, understand the structure of the data, and gain insights that may not be apparent at first glance.</p>
<p>This blog post will guide you through the fundamental steps and important factors to consider when performing two common types of clustering: Hierarchical clustering and K-means clustering. You’ll learn how to apply these techniques to group similar data points and uncover patterns in your data.</p>
</section>
<section id="example-dataset" class="level1">
<h1>Example dataset</h1>
<p>The dataset used in this demonstration consists of ratings provided by 240 participants. These participants were randomly assigned to one of five conditions, each using a different scale to measure valence. The scales varied in the word pairs used to describe valence, such as unpleasant-pleasant, negative-positive, bad-good, dislike-like, and displeasing-pleasing.</p>
<p>Regardless of the assigned condition, each participant rated the valence of 27 different emotion words. The goal of this analysis is to explore how these 27 emotions cluster together based on the valence ratings, without taking the condition into account.</p>
<p>To better understand how emotions are related to each other based on valence ratings, we will use two different clustering techniques: k-means and hierarchical clustering. These methods will allow us to group similar emotions together and uncover any patterns or underlying dimensions of emotion within the dataset. By analyzing the clusters formed by these techniques, we can gain valuable insights into how different emotions are perceived and how they relate to one another in terms of their valence.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">"https://raw.githubusercontent.com/hennavartiainen/blog_post/master/blog_data.csv"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>data_original <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(url)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data_original)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  SubjID  emotion   rating    condition
1      1    SAFE  80.94744 Dislike-Like
2      1 RELAXED  80.94744 Dislike-Like
3      1 WORRIED  11.09959 Dislike-Like
4      1  LONELY  11.09959 Dislike-Like
5      1 PLEASED  75.76072 Dislike-Like
6      1  SCARED  11.27248 Dislike-Like</code></pre>
</div>
</div>
</section>
<section id="preprocessing" class="level1">
<h1>Preprocessing</h1>
<p>To prepare the dataset for clustering, we need to format it in a specific way.</p>
<ul>
<li>Formatting the dataset: We will arrange the data so that each emotion is represented as a row and each participant as a column. This way, the clustering algorithm will group emotions based on their patterns of valence ratings across participants. If we reversed the order, the clusters would provide insights into different subgroups or profiles of participants based on their emotional responses.</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load original data</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data_original <span class="sc">%&gt;%</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the relevant columns</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(SubjID, emotion, rating) <span class="sc">%&gt;%</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Move emotions to columns, and SubjID and condition as rows</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">dcast</span>(emotion  <span class="sc">~</span> SubjID, <span class="at">value.var =</span> <span class="st">"rating"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make emotion names the row names</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rownames</span>(data) <span class="ot">&lt;-</span> data<span class="sc">$</span>emotion</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove unnecessary columns</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    data <span class="ot">&lt;-</span> data[, <span class="sc">!</span><span class="fu">names</span>(data) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"SubjID"</span>, <span class="st">"emotion"</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Standardize variables (if not on same scale)</li>
</ul>
<p>If your dataset contains variables that are not on the same scale, it is important to standardize them before performing clustering. Standardization ensures that each variable contributes equally to the clustering process, regardless of its original scale.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize the entire dataset</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">scale</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Deal with missing data</li>
</ul>
<p>Dealing with missing data is an important step in data preprocessing. Here are some common options for handling missing data:</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Complete Case Analysis</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">Mean/Median/Mode Imputation</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false" href="">Multiple Imputation</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<p>This approach involves removing any observations or rows that have missing values. It is suitable when the missing data is minimal and randomly distributed across the dataset. In R, you can use the na.omit() function to remove rows with missing values:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove missing data</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<p>Missing values can be replaced with the mean, median, or mode of the corresponding variable. This method assumes that the missing values are missing at random and that the replacement values adequately represent the missing information. In R, you can use the mean(), median(), or Mode() function from appropriate packages to calculate the respective measures and then fill in the missing values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of each column</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>column_means <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(data, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace missing values with the column means</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>data_imputed <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">is.na</span>(data), column_means[<span class="fu">col</span>(data)], data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<p>Multiple imputation is a technique where missing values are imputed multiple times based on the observed data distribution. Multiple complete datasets are created, and the analysis is performed on each dataset separately. The results are then combined to generate final estimates. The mice package in R provides functions for multiple imputation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the mice package</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mice)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the imputation model</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>imputation_model <span class="ot">&lt;-</span> <span class="fu">mice</span>(data, <span class="at">m =</span> <span class="dv">5</span>, <span class="at">method =</span> <span class="st">"pmm"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the imputation</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>imputed_data <span class="ot">&lt;-</span> <span class="fu">complete</span>(imputation_model)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Access the imputed datasets</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>imputed_datasets <span class="ot">&lt;-</span> imputation_model<span class="sc">$</span>imp</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform analysis on each imputed dataset</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(imputed_datasets)) {</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  imputed_analysis <span class="ot">&lt;-</span> <span class="fu">lm</span>(dependent_variable <span class="sc">~</span> independent_variable, <span class="at">data =</span> imputed_datasets[[i]])</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Perform further analysis or save the results</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine the results from multiple imputations</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>combined_results <span class="ot">&lt;-</span> <span class="fu">pool</span>(imputation_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="hierarchical-clustering" class="level1">
<h1>Hierarchical clustering</h1>
<p>Hierarchical clustering is a process of merging similar clusters to create a hierarchy. Here’s how it works:</p>
<ol type="1">
<li>Start with each data point as a separate cluster.</li>
<li>Measure the similarity between clusters and merge the most similar ones.</li>
<li>Repeat step 2 until all data points belong to a single cluster.</li>
<li>The result is a dendrogram, showing the hierarchy of clusters.</li>
<li>To determine the number of clusters, choose a level on the dendrogram and cut horizontally.</li>
</ol>
<p>By iteratively merging clusters, we can visualize how data points group together and decide how many clusters to obtain from the dendrogram.</p>
<section id="calculating-the-dissimilarity-matrix" class="level2">
<h2 class="anchored" data-anchor-id="calculating-the-dissimilarity-matrix">1. Calculating the dissimilarity matrix</h2>
<p>The next step in hierarchical clustering is to calculate the dissimilarity matrix. This matrix represents the distances between pairs of data points and plays a crucial role in determining how the clusters are formed. There are various distance metrics to choose from, each capturing different aspects of similarity or dissimilarity between data points.</p>
<p>To illustrate these distance metrics, let’s consider a simple example with two-dimensional data points:</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true" href="">Euclidean distance</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false" href="">Manhattan Distance</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>This metric measures the straight-line distance between two data points in a multi-dimensional space. It considers the differences between each feature of the data points and calculates the overall distance. It is well-suited for datasets where the magnitude and scale of features matter.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p>Manhattan distance measures the sum of the absolute differences between corresponding features of two data points. It calculates the distance by moving horizontally and vertically along the axes, as if navigating through city blocks. It is suitable for datasets where only the differences between features matter, regardless of their magnitude.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
<p>Once you have determined which distance metric is appropriate for your data, you can calculate the dissimilarity matrix. The dissimilarity matrix represents the pairwise distances or similarities between data points and is a fundamental component of clustering algorithms. In this case, we will use the dissimilarity matrix to capture the similarities or differences in valence ratings between the 27 emotion words.</p>
<p>Here’s how you can calculate the dissimilarity matrix using either Euclidean or Manhattan distance and visualize it as a heatmap:</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true" href="">Euclidean distance</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false" href="">Manhattan distance</a></li></ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate dissimilarity matrix</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>distance_matrix_euclidean <span class="ot">&lt;-</span> <span class="fu">dist</span>(data, <span class="at">method =</span> <span class="st">"euclidean"</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the distance matrix to a square matrix</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>distance_matrix_euclidean_plot <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(distance_matrix_euclidean)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a heatmap of the distance matrix</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="fu">heatmap</span>(distance_matrix_euclidean_plot, </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Euclidean Distance Matrix Heatmap"</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">heat.colors</span>(<span class="dv">256</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate dissimilarity matrix</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>distance_matrix_manhattan <span class="ot">&lt;-</span> <span class="fu">dist</span>(data, <span class="at">method =</span> <span class="st">"manhattan"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the distance matrix to a square matrix</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>distance_matrix_manhattan_plot <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(distance_matrix_manhattan)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot a heatmap of the distance matrix</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="fu">heatmap</span>(distance_matrix_manhattan_plot, </span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Manhattan Distance Matrix Heatmap"</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="fu">heat.colors</span>(<span class="dv">256</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="determining-the-linkage-type" class="level2">
<h2 class="anchored" data-anchor-id="determining-the-linkage-type">2. Determining the linkage type</h2>
<p>In hierarchical clustering, the choice of linkage method determines how the distance between clusters is calculated when constructing the hierarchical structure. Different linkage methods can lead to different cluster structures. Here are some commonly used linkage methods in hierarchical clustering:</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true" href="">Single linkage</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false" href="">Complete linkage</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-3" role="tab" aria-controls="tabset-4-3" aria-selected="false" href="">Average Linkage</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-4" role="tab" aria-controls="tabset-4-4" aria-selected="false" href="">Ward’s Method</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-5-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-5" role="tab" aria-controls="tabset-4-5" aria-selected="false" href="">Centroid</a></li></ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<p>In single linkage, the distance between two clusters is defined as the minimum distance between any two points in the two clusters. This method tends to form elongated, chain-like clusters and is sensitive to noise and outliers.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<p>In complete linkage, the distance between two clusters is calculated as the maximum distance between any two points in the two clusters. It is less sensitive to outliers compared to single linkage, meaning that outliers have less influence on the clustering results. Complete linkage is a good choice when dealing with data that contains outliers or when the desire is to form well-separated and distinct clusters. However, it may struggle with identifying clusters that have varying sizes or shapes.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-4-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-3-tab">
<p>In average linkage, the distance between two clusters is calculated as the average distance between all pairs of points in the two clusters. This method aims to strike a balance between the extreme approaches of single and complete linkage. By considering the average distance, it takes into account the overall similarity between clusters without being overly influenced by individual points. It is a popular choice when the dataset contains diverse cluster shapes and sizes, as it can capture both local and global similarities. However, it may still be sensitive to outliers to some extent.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-4-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-4-tab">
<p>Ward’s method is a linkage method in hierarchical clustering that focuses on minimizing the within-cluster variance. It calculates the distance between two clusters by considering the increase in the total within-cluster sum of squares when merging them. The idea is to merge clusters in a way that results in the smallest increase in variance within each cluster. In the visualization below, you can see that the first one (two clusters merged together) has high variance, whereas the second option is better.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-14-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-4-5" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-5-tab">
<p>In centroid linkage, the distance between two clusters is defined as the Euclidean distance between their centroids (mean vectors). This method is based on the idea that clusters with closer centroids are more similar.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="running-hierarchical-cluster-analysis" class="level2">
<h2 class="anchored" data-anchor-id="running-hierarchical-cluster-analysis">3. Running hierarchical cluster analysis</h2>
<p>Reiterating the information above, for the purposes of this tutorial, we will use the Euclidean distance metric and Ward’s method for linkage in our cluster analysis. We can perform the analysis and visualize the results using the following code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>hclust_avg <span class="ot">&lt;-</span> <span class="fu">hclust</span>(distance_matrix_euclidean, <span class="at">method =</span> <span class="st">'ward.D2'</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hclust_avg, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The dendrogram visually shows how emotion words are grouped based on their valence ratings. It helps us determine the number of clusters by choosing a horizontal cut. Lower cuts indicate more similar clusters, while higher cuts represent more dissimilar clusters. The dendrogram suggests the presence of two distinct clusters within the data, indicated by the long vertical lines. This indicates that there are two groups of emotion words that exhibit similar patterns of valence ratings.</p>
</section>
<section id="visualization" class="level2">
<h2 class="anchored" data-anchor-id="visualization">4. Visualization</h2>
<p>We can also visualize the two clusters in two different colors, using the code below:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_dend</span>(<span class="at">x =</span> hclust_avg, <span class="at">cex =</span> <span class="fl">0.8</span>, <span class="at">lwd =</span> <span class="fl">0.8</span>, <span class="at">k =</span> <span class="dv">2</span>,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>          <span class="at">rect =</span> <span class="cn">TRUE</span>, </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">rect_border =</span> <span class="st">"gray"</span>, </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>          <span class="at">rect_fill =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
</section>
<section id="k-means" class="level1">
<h1>K-means</h1>
<p>K-means clustering is a method that helps us group similar data points together by repeatedly updating cluster centers. Here’s how it works:</p>
<ol type="1">
<li>Start by randomly choosing some points as the initial cluster centers.</li>
<li>Calculate the distance between each data point and the cluster centers.</li>
<li>Assign each data point to the nearest cluster center based on distance.</li>
<li>Recalculate the cluster centers by taking the average of all the data points assigned to each cluster.</li>
<li>Repeat steps 2 to 4 until the cluster assignments and centers stop changing significantly.</li>
<li>At the end, each data point belongs to a specific cluster, and the cluster centers represent the average positions of the clusters.</li>
</ol>
<p>K-means clustering helps us find groups of similar data points based on their distances from the cluster centers. It’s a useful technique for discovering patterns and organizing data into meaningful clusters.</p>
<section id="determining-the-optimal-number-of-clusters" class="level2">
<h2 class="anchored" data-anchor-id="determining-the-optimal-number-of-clusters">1. Determining the optimal number of clusters</h2>
<p>Determining the optimal number of clusters is an important step in k-means clustering. It helps us find the right balance between capturing meaningful patterns and avoiding overfitting. Here are a few methods commonly used to determine the optimal number of clusters:</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true" href="">Elbow method</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false" href="">Silhouette method</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-3" role="tab" aria-controls="tabset-5-3" aria-selected="false" href="">Gap statistic</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-4" role="tab" aria-controls="tabset-5-4" aria-selected="false" href="">Hartigan method</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-5-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-5" role="tab" aria-controls="tabset-5-5" aria-selected="false" href="">Combination</a></li></ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<p>The elbow method helps us find the optimal number of clusters in a dataset. We plot the number of clusters (k) against a measure of clustering quality, such as the within-cluster sum of squares (WSS). The plot resembles an arm, and we look for the “elbow” point where the plot starts to flatten out. This indicates the optimal number of clusters.</p>
<p>The interpretation is simple: choose the value of k at the elbow point. It captures the most meaningful information without adding unnecessary complexity. The elbow point represents a balance between capturing patterns and avoiding overfitting. So, the optimal number of clusters is often determined by this point on the plot.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(data, <span class="at">FUN =</span> hcut, <span class="at">method =</span> <span class="st">"wss"</span>, </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">k.max =</span> <span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Elbow method"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<p>The silhouette method helps us find the optimal number of clusters in a dataset. It evaluates clustering quality by measuring how well each data point fits within its assigned cluster compared to other clusters.</p>
<p>In the plot, we look for the highest point on the curve. This indicates the optimal number of clusters that maximizes the quality of clustering.</p>
<p>A higher value at the peak suggests well-separated and distinct clusters, while a lower value indicates overlapping or poorly separated clusters.</p>
<p>By choosing the peak point, we strike a balance between having meaningful clusters and avoiding unnecessary complexity.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(data, <span class="at">FUN =</span> hcut, <span class="at">method =</span> <span class="st">"silhouette"</span>, </span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">k.max =</span> <span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Silhouette method"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-5-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-3-tab">
<p>The gap statistic compares the differences between the clusters we observe and what we would expect if there were no clusters. In the plot, we want to find the largest gap. This tells us the optimal number of clusters. We also look for gaps that are much bigger than the others, as they indicate good choices for the number of clusters.</p>
<p>The gap statistic plot helps us see where the clusters we observe are different from what we would expect by chance. It helps us find the right number of clusters that capture the true structure in the data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(data, <span class="at">FUN =</span> hcut, <span class="at">method =</span> <span class="st">"gap_stat"</span>, </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">k.max =</span> <span class="dv">10</span>, <span class="at">nboot=</span><span class="dv">100</span>) <span class="sc">+</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"Gap statistic"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-5-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-4-tab">
<p>The Hartigan method helps us determine the best number of clusters in a dataset. It measures the decrease in within-cluster sum of squares (WCSS) as the number of clusters increases.</p>
<p>In the plot, we look for a significant drop in the Hartigan’s Index. This drop indicates the optimal number of clusters. We want to find the point where adding more clusters doesn’t significantly improve the WCSS.</p>
<p>The Hartigan method helps us identify the number of clusters where the improvement in clustering quality becomes less substantial. However, it’s important to consider other evaluation methods and domain knowledge to make an informed decision about the optimal number of clusters.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(useful)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>clusters_best <span class="ot">&lt;-</span> <span class="fu">FitKMeans</span>(data, <span class="at">max.clusters=</span><span class="dv">10</span>, <span class="at">nstart=</span><span class="dv">5</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">PlotHartigan</span>(clusters_best)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
<div id="tabset-5-5" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-5-tab">
<p>The combination method is a comprehensive approach that combines multiple clustering validation measures to determine the optimal number of clusters. It calculates various indices for different values of k and provides a recommendation based on their results. The plot visualizes the index values, helping to identify the optimal number of clusters where the indices are maximized or stable. This method reduces bias and provides a robust assessment of clustering quality.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(parameters)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">n_clusters</span>(</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  data,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">standardize =</span> <span class="cn">FALSE</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">include_factors =</span> <span class="cn">FALSE</span>,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">package =</span> <span class="fu">c</span>(<span class="st">"all"</span>),</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">fast =</span> <span class="cn">TRUE</span>,</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">nbclust_method =</span> <span class="st">"kmeans"</span>,</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">n_max =</span> <span class="dv">5</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="running-k-means-analysis" class="level2">
<h2 class="anchored" data-anchor-id="running-k-means-analysis">2. Running k-means analysis</h2>
<p>Most of the methods above suggest that the data includes 2 clusters. Therefore, the analyses will be conducted with k = 2.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(data, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualization-1" class="level2">
<h2 class="anchored" data-anchor-id="visualization-1">3. Visualization</h2>
<p>There are many ways to visualize the assigned cluster labels provided by the code above, here’s one example:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stats)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(result,data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>